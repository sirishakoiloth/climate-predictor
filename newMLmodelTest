import pandas as pd
import numpy as np
from sklearn.model_selection import TimeSeriesSplit
from sklearn.linear_model import Ridge
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score # Added MAE and R2
from sklearn.preprocessing import MinMaxScaler
import json
import os
import warnings

warnings.filterwarnings('ignore', category=FutureWarning)

# --- Configuration ---
TEMP_MONTHLY_FILE = 'data/raw/average_monthly_temperature_by_state_1950-2022.csv'
TEMP_YEARLY_FILE = 'data/raw/climdiv_state_year.csv'
FEMA_FILE = 'data/raw/DisasterDeclarationsSummaries.csv' # Make sure this filename is correct
OUTPUT_JS_FILE = 'climate_risk_data.js'
N_CV_SPLITS = 5
PREDICTION_HORIZON_YEARS = 10

# Create output directory if it doesn't exist
output_dir = os.path.dirname(OUTPUT_JS_FILE)
if output_dir and not os.path.exists(output_dir):
    os.makedirs(output_dir)

# FIPS to state abbreviation mapping
fips_to_state_abbr = {
    "01": "AL", "02": "AK", "04": "AZ", "05": "AR", "06": "CA", "08": "CO", "09": "CT",
    "10": "DE", "11": "DC", "12": "FL", "13": "GA", "15": "HI", "16": "ID", "17": "IL",
    "18": "IN", "19": "IA", "20": "KS", "21": "KY", "22": "LA", "23": "ME", "24": "MD",
    "25": "MA", "26": "MI", "27": "MN", "28": "MS", "29": "MO", "30": "MT", "31": "NE",
    "32": "NV", "33": "NH", "34": "NJ", "35": "NM", "36": "NY", "37": "NC", "38": "ND",
    "39": "OH", "40": "OK", "41": "OR", "42": "PA", "44": "RI", "45": "SC", "46": "SD",
    "47": "TN", "48": "TX", "49": "UT", "50": "VT", "51": "VA", "53": "WA", "54": "WV",
    "55": "WI", "56": "WY"
}
state_abbr_to_fips = {v: k for k, v in fips_to_state_abbr.items()}


# --- 1. Load Data ---
print("Loading data...")
try:
    df_temp_yearly = pd.read_csv(TEMP_YEARLY_FILE, index_col='index')
    # Check FEMA file columns - needs 'state', 'declarationDate', 'incidentType'
    df_fema = pd.read_csv(FEMA_FILE)
    # Verify required columns exist
    required_fema_cols = ['state', 'declarationDate', 'incidentType']
    if not all(col in df_fema.columns for col in required_fema_cols):
        print(f"Error: FEMA file '{FEMA_FILE}' is missing one or more required columns: {required_fema_cols}")
        print(f"Available columns: {df_fema.columns.tolist()}")
        exit()

except FileNotFoundError as e:
    print(f"Error loading file: {e}")
    print(f"Please ensure '{TEMP_YEARLY_FILE}' and '{FEMA_FILE}' are in the 'data/raw/' directory.")
    exit()
except Exception as e:
    print(f"An error occurred during data loading: {e}")
    exit()


# --- 2. Preprocess Data ---
print("Preprocessing data...")
# -- Temperature Data --
df_temp_yearly['fips'] = df_temp_yearly['fips'].astype(str).str.zfill(2)
df_temp_yearly['state_abbr'] = df_temp_yearly['fips'].map(fips_to_state_abbr)
df_temp_yearly = df_temp_yearly.dropna(subset=['state_abbr'])
baseline_start_year = 1950
baseline_end_year = 2000
state_baselines = df_temp_yearly[
    (df_temp_yearly['year'] >= baseline_start_year) &
    (df_temp_yearly['year'] <= baseline_end_year)
].groupby('state_abbr')['tempc'].mean().to_dict()
df_temp_yearly['tempc_anomaly'] = df_temp_yearly.apply(
    lambda row: row['tempc'] - state_baselines.get(row['state_abbr'], np.nan), axis=1
)
df_temp_yearly = df_temp_yearly.sort_values(by=['state_abbr', 'year'])

# -- FEMA Disaster Data --
try:
    df_fema['declarationDate'] = pd.to_datetime(df_fema['declarationDate'], errors='coerce') # Handle potential parsing errors
    df_fema = df_fema.dropna(subset=['declarationDate']) # Drop rows where date couldn't be parsed
    df_fema['year'] = df_fema['declarationDate'].dt.year
    df_fema = df_fema.rename(columns={'state': 'state_abbr'})
    # Ensure state_abbr exists and filter for valid ones if needed
    df_fema = df_fema[df_fema['state_abbr'].isin(state_abbr_to_fips.keys())]

    disaster_counts = df_fema.groupby(['state_abbr', 'year']).size().reset_index(name='disaster_count')

    recent_years_for_top_disaster = 5
    # Handle case where FEMA data might be older than temp data
    current_max_year_fema = df_fema['year'].max() if not df_fema.empty else baseline_start_year -1 # Avoid error if df_fema is empty

    df_fema_recent = df_fema[df_fema['year'] > current_max_year_fema - recent_years_for_top_disaster]

    # Aggregate top disasters carefully, handling empty groups
    top_disasters_agg = df_fema_recent.groupby('state_abbr')['incidentType'].agg(
        lambda x: x.mode()[0] if not x.mode().empty else 'N/A'
    )
    top_disasters = top_disasters_agg.reset_index()
    top_disasters = top_disasters.rename(columns={'incidentType': 'top_disaster_type'})

except KeyError as e:
    print(f"Error processing FEMA data: Missing column {e}. Please check '{FEMA_FILE}'.")
    exit()
except Exception as e:
    print(f"An error occurred during FEMA data preprocessing: {e}")
    exit()


# -- Merge Data --
print("Merging datasets...")
df_merged = pd.merge(
    df_temp_yearly[['state_abbr', 'year', 'tempc', 'tempc_anomaly']],
    disaster_counts, on=['state_abbr', 'year'], how='left'
)
df_merged['disaster_count'] = df_merged['disaster_count'].fillna(0).astype(int)
df_merged = df_merged.sort_values(by=['state_abbr', 'year'])


# --- 3. Feature Engineering ---
print("Engineering features for ML...")
df_merged['tempc_anomaly_prev_year'] = df_merged.groupby('state_abbr')['tempc_anomaly'].shift(1)
df_merged['disaster_count_prev_year'] = df_merged.groupby('state_abbr')['disaster_count'].shift(1)
df_merged['target_tempc_anomaly_next_year'] = df_merged.groupby('state_abbr')['tempc_anomaly'].shift(-1)

df_model_data = df_merged.dropna(subset=[
    'tempc_anomaly', 'tempc_anomaly_prev_year', 'disaster_count',
    'disaster_count_prev_year', 'target_tempc_anomaly_next_year'
]).copy()

if df_model_data.empty:
    print("Error: No data available for modeling after feature engineering and NaN removal.")
    print("This might be due to insufficient overlapping data between temperature and disaster datasets.")
    exit()

# Sort by year for TimeSeriesSplit
df_model_data = df_model_data.sort_values(by='year')

features = ['year', 'tempc_anomaly', 'tempc_anomaly_prev_year', 'disaster_count', 'disaster_count_prev_year']
target = 'target_tempc_anomaly_next_year'

X = df_model_data[features]
y = df_model_data[target]

# --- 4. Machine Learning Model & Time Series Cross-Validation ---
print(f"\nPerforming Time Series Cross-Validation with {N_CV_SPLITS} splits...")

tscv = TimeSeriesSplit(n_splits=N_CV_SPLITS)
model = Ridge(alpha=1.0) # Alpha=1.0 is a common default, adjust if needed

# Store scores for each fold
rmse_scores = []
mae_scores = []
r2_scores = []

fold = 0
for train_index, val_index in tscv.split(X):
    fold += 1
    X_train, X_val = X.iloc[train_index], X.iloc[val_index]
    y_train, y_val = y.iloc[train_index], y.iloc[val_index]

    # Handle potential NaNs (median imputation)
    X_train_median = X_train.median() # Calculate median on train split only
    X_train = X_train.fillna(X_train_median)
    X_val = X_val.fillna(X_train_median) # Apply train median to val split

    model.fit(X_train, y_train)
    y_pred_val = model.predict(X_val)

    # Calculate metrics for this fold
    rmse = np.sqrt(mean_squared_error(y_val, y_pred_val))
    mae = mean_absolute_error(y_val, y_pred_val)
    r2 = r2_score(y_val, y_pred_val)

    rmse_scores.append(rmse)
    mae_scores.append(mae)
    r2_scores.append(r2)

    print(f"Fold {fold}/{N_CV_SPLITS} - Val RMSE: {rmse:.4f}, Val MAE: {mae:.4f}, Val R²: {r2:.4f}")

# Calculate average and std dev of scores across folds
avg_cv_rmse = np.mean(rmse_scores)
std_cv_rmse = np.std(rmse_scores)
avg_cv_mae = np.mean(mae_scores)
std_cv_mae = np.std(mae_scores)
avg_cv_r2 = np.mean(r2_scores)
std_cv_r2 = np.std(r2_scores)

print("\n--- Cross-Validation Summary ---")
print(f"Average RMSE: {avg_cv_rmse:.4f} (+/- {std_cv_rmse:.4f})")
print(f"Average MAE:  {avg_cv_mae:.4f} (+/- {std_cv_mae:.4f})")
print(f"Average R²:   {avg_cv_r2:.4f} (+/- {std_cv_r2:.4f})")
print("---------------------------------")

# --- Retrain final model on ALL available data ---
print("\nRetraining final model on all available data...")
X_full_median = X.median() # Calculate median on the full dataset
X = X.fillna(X_full_median)
model.fit(X, y)
print("Final model trained.")


# --- 5. Iterative Prediction for Next Decade & Risk Calculation ---
print(f"\nPredicting next {PREDICTION_HORIZON_YEARS} years' anomaly and calculating risk...")

last_year_data_full = df_merged.loc[df_merged.groupby('state_abbr')['year'].idxmax()]
last_known_year = last_year_data_full['year'].max()
print(f"Last known year in data: {last_known_year}")
print(f"Predicting from {last_known_year + 1} to {last_known_year + PREDICTION_HORIZON_YEARS}")

future_predictions = {}
current_features = last_year_data_full.set_index('state_abbr')[[
    'year', 'tempc_anomaly', 'tempc_anomaly_prev_year',
    'disaster_count', 'disaster_count_prev_year'
]].copy()

# Fill NaNs in initial features using medians from the full training set (X)
for col in features:
    if current_features[col].isnull().any():
        fill_value = X_full_median[col] # Use median calculated before final training
        print(f"Warning: Filling initial NaN in '{col}' with median {fill_value:.2f}")
        current_features[col] = current_features[col].fillna(fill_value)

predicted_anomalies_decade = {}
all_states_in_prediction = current_features.index.unique()

for i in range(PREDICTION_HORIZON_YEARS):
    predict_yr = last_known_year + 1 + i
    current_features['year'] = predict_yr
    X_step = current_features[features].fillna(X_full_median) # Ensure no NaNs passed to predict

    step_predictions = model.predict(X_step)
    predicted_anomaly_series = pd.Series(step_predictions, index=X_step.index)
    future_predictions[predict_yr] = predicted_anomaly_series

    next_tempc_anomaly_prev = current_features['tempc_anomaly'].copy()
    current_features['tempc_anomaly'] = predicted_anomaly_series
    current_features['tempc_anomaly_prev_year'] = next_tempc_anomaly_prev

    if i == PREDICTION_HORIZON_YEARS - 1:
        predicted_anomalies_decade = predicted_anomaly_series.copy()

predicted_anomalies_next_year = future_predictions[last_known_year + 1]

# --- Risk Score Calculation ---
recent_disaster_avg = df_merged[df_merged['year'] > last_known_year - 5]\
    .groupby('state_abbr')['disaster_count'].mean().fillna(0)

risk_components = pd.DataFrame({
    'predicted_anomaly': predicted_anomalies_next_year,
    'recent_disasters': recent_disaster_avg
}).reindex(fips_to_state_abbr.values()).fillna(0)

scaler = MinMaxScaler()
risk_components_scaled = scaler.fit_transform(risk_components)
risk_components_scaled = pd.DataFrame(risk_components_scaled,
                                      columns=['anomaly_scaled', 'disasters_scaled'],
                                      index=risk_components.index)

weight_anomaly = 0.6
weight_disasters = 0.4
risk_components_scaled['risk_score'] = (weight_anomaly * risk_components_scaled['anomaly_scaled'] +
                                        weight_disasters * risk_components_scaled['disasters_scaled'])
risk_components_scaled['risk_score'] = (risk_components_scaled['risk_score'] * 100).round(1)

quantiles = risk_components_scaled['risk_score'].quantile([0.2, 0.4, 0.6, 0.8]).to_dict()
# Check for non-unique quantile values which can happen with sparse/uniform data
unique_quantiles = sorted(list(set(quantiles.values())))
if len(unique_quantiles) < 4:
    print("\nWarning: Risk score quantiles are not unique. Adjusting categories based on available breaks.")
    # Simplified categorization if quantiles are collapsed
    if len(unique_quantiles) == 1: # Only one value
        quantiles[0.2]=quantiles[0.4]=quantiles[0.6]=quantiles[0.8]=unique_quantiles[0]
    elif len(unique_quantiles) == 2: # Two values
        quantiles[0.2]=quantiles[0.4]=unique_quantiles[0]
        quantiles[0.6]=quantiles[0.8]=unique_quantiles[1]
    elif len(unique_quantiles) == 3: # Three values
         quantiles[0.2]=unique_quantiles[0]
         quantiles[0.4]=quantiles[0.6]=unique_quantiles[1]
         quantiles[0.8]=unique_quantiles[2]

def get_risk_category_and_color(score, q_map):
    q_vals = sorted(q_map.values()) # Use potentially adjusted quantiles
    # Use <= for ranges to ensure every value falls into a category
    if score <= q_vals[0]: return "Very Low", "#4575b4"
    elif score <= q_vals[1]: return "Low", "#91bfdb"
    elif score <= q_vals[2]: return "Medium", "#ffffbf"
    elif score <= q_vals[3]: return "High", "#fc8d59"
    else: return "Very High", "#d73027"

risk_categories_colors = risk_components_scaled['risk_score'].apply(get_risk_category_and_color, q_map=quantiles)
risk_components_scaled['riskCategory'] = risk_categories_colors.apply(lambda x: x[0])
risk_components_scaled['color'] = risk_categories_colors.apply(lambda x: x[1])


# --- 6. Output Generation ---
print(f"\nGenerating JavaScript output file: {OUTPUT_JS_FILE}")
climate_risk_data_dict = {}
all_states = fips_to_state_abbr.values()

for state_abbr in all_states:
    state_data = {
        "riskScore": "N/A",
        "riskCategory": "No Data",
        "color": "#333333",
        "topDisasterType": "No Data",
        "totalDisasters": 0,
        "nextYearTrend": "N/A",
        "decadeAnomalyPrediction": "N/A"
    }

    if state_abbr in risk_components_scaled.index:
        risk_info = risk_components_scaled.loc[state_abbr]
        # Check for NaN risk score before assigning
        state_data["riskScore"] = risk_info['risk_score'] if pd.notna(risk_info['risk_score']) else "N/A"
        state_data["riskCategory"] = risk_info['riskCategory']
        state_data["color"] = risk_info['color']

    if state_abbr in predicted_anomalies_next_year.index:
        trend_val = predicted_anomalies_next_year.loc[state_abbr]
        state_data["nextYearTrend"] = f"{trend_val:+.2f}°C Anomaly (1yr)" if pd.notna(trend_val) else "N/A"

    if state_abbr in predicted_anomalies_decade.index:
        decade_val = predicted_anomalies_decade.loc[state_abbr]
        state_data["decadeAnomalyPrediction"] = f"{decade_val:+.2f}°C Anomaly ({PREDICTION_HORIZON_YEARS}yr)" if pd.notna(decade_val) else "Prediction Error"

    if state_abbr in top_disasters['state_abbr'].values:
         state_data["topDisasterType"] = top_disasters.loc[top_disasters['state_abbr'] == state_abbr, 'top_disaster_type'].iloc[0]

    # Ensure df_fema is filtered correctly before counting
    if not df_fema.empty and 'state_abbr' in df_fema.columns:
        total_hist_disasters = df_fema[df_fema['state_abbr'] == state_abbr].shape[0]
        state_data["totalDisasters"] = total_hist_disasters
    else:
        state_data["totalDisasters"] = 0 # Or N/A if preferred

    climate_risk_data_dict[state_abbr] = state_data

# Convert state data dictionary to JSON
json_string = json.dumps(climate_risk_data_dict, indent=4)

# Create JavaScript output string including the state data AND the validation RMSE
# Added a separate variable for the validation metric
js_output = f"""
// Climate risk data for each state
const climateRiskData = {json_string};

// Average Root Mean Squared Error from Time Series Cross-Validation
const validationRMSE = {avg_cv_rmse:.4f};
"""

# Write to file
try:
    with open(OUTPUT_JS_FILE, 'w') as f:
        f.write(js_output)
    print(f"Successfully created {OUTPUT_JS_FILE}")
except IOError as e:
    print(f"Error writing to file {OUTPUT_JS_FILE}: {e}")

print("\nScript finished.")