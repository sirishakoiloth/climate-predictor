import pandas as pd
import numpy as np
from sklearn.model_selection import TimeSeriesSplit # Changed from train_test_split
from sklearn.linear_model import Ridge
from sklearn.metrics import mean_squared_error
from sklearn.preprocessing import MinMaxScaler
import json
import os
import warnings

warnings.filterwarnings('ignore', category=FutureWarning)

# --- Configuration ---
TEMP_MONTHLY_FILE = 'data/raw/average_monthly_temperature_by_state_1950-2022.csv'
TEMP_YEARLY_FILE = 'data/raw/climdiv_state_year.csv'
FEMA_FILE = 'data/raw/DisasterDeclarationsSummaries.csv'
OUTPUT_JS_FILE = 'climate_risk_data.js'
N_CV_SPLITS = 5 # Number of splits for Time Series Cross-Validation
PREDICTION_HORIZON_YEARS = 10 # Predict this many years into the future

# Create output directory if it doesn't exist
output_dir = os.path.dirname(OUTPUT_JS_FILE)
if output_dir and not os.path.exists(output_dir):
    os.makedirs(output_dir)

# FIPS to state abbreviation mapping
fips_to_state_abbr = {
    "01": "AL", "02": "AK", "04": "AZ", "05": "AR", "06": "CA", "08": "CO", "09": "CT",
    "10": "DE", "11": "DC", "12": "FL", "13": "GA", "15": "HI", "16": "ID", "17": "IL",
    "18": "IN", "19": "IA", "20": "KS", "21": "KY", "22": "LA", "23": "ME", "24": "MD",
    "25": "MA", "26": "MI", "27": "MN", "28": "MS", "29": "MO", "30": "MT", "31": "NE",
    "32": "NV", "33": "NH", "34": "NJ", "35": "NM", "36": "NY", "37": "NC", "38": "ND",
    "39": "OH", "40": "OK", "41": "OR", "42": "PA", "44": "RI", "45": "SC", "46": "SD",
    "47": "TN", "48": "TX", "49": "UT", "50": "VT", "51": "VA", "53": "WA", "54": "WV",
    "55": "WI", "56": "WY"
}
state_abbr_to_fips = {v: k for k, v in fips_to_state_abbr.items()}


# --- 1. Load Data ---
print("Loading data...")
try:
    df_temp_yearly = pd.read_csv(TEMP_YEARLY_FILE, index_col='index')
    df_fema = pd.read_csv(FEMA_FILE)
except FileNotFoundError as e:
    print(f"Error loading file: {e}")
    print("Please ensure the CSV files are in the 'data/raw/' directory.")
    exit()

# --- 2. Preprocess Data ---
print("Preprocessing data...")
# -- Temperature Data --
df_temp_yearly['fips'] = df_temp_yearly['fips'].astype(str).str.zfill(2)
df_temp_yearly['state_abbr'] = df_temp_yearly['fips'].map(fips_to_state_abbr)
df_temp_yearly = df_temp_yearly.dropna(subset=['state_abbr'])
baseline_start_year = 1950
baseline_end_year = 2000
state_baselines = df_temp_yearly[
    (df_temp_yearly['year'] >= baseline_start_year) &
    (df_temp_yearly['year'] <= baseline_end_year)
].groupby('state_abbr')['tempc'].mean().to_dict()
df_temp_yearly['tempc_anomaly'] = df_temp_yearly.apply(
    lambda row: row['tempc'] - state_baselines.get(row['state_abbr'], np.nan), axis=1
)
df_temp_yearly = df_temp_yearly.sort_values(by=['state_abbr', 'year'])

# -- FEMA Disaster Data --
df_fema['declarationDate'] = pd.to_datetime(df_fema['declarationDate'])
df_fema['year'] = df_fema['declarationDate'].dt.year
df_fema = df_fema.rename(columns={'state': 'state_abbr'})
disaster_counts = df_fema.groupby(['state_abbr', 'year']).size().reset_index(name='disaster_count')
recent_years_for_top_disaster = 5
current_max_year_fema = df_fema['year'].max()
df_fema_recent = df_fema[df_fema['year'] > current_max_year_fema - recent_years_for_top_disaster]
top_disasters = df_fema_recent.groupby('state_abbr')['incidentType'].agg(lambda x: x.mode()[0] if not x.mode().empty else 'N/A').reset_index()
top_disasters = top_disasters.rename(columns={'incidentType': 'top_disaster_type'})

# -- Merge Data --
print("Merging datasets...")
df_merged = pd.merge(
    df_temp_yearly[['state_abbr', 'year', 'tempc', 'tempc_anomaly']],
    disaster_counts, on=['state_abbr', 'year'], how='left'
)
df_merged['disaster_count'] = df_merged['disaster_count'].fillna(0).astype(int)
# Sort essential for time series features and splitting
df_merged = df_merged.sort_values(by=['state_abbr', 'year'])


# --- 3. Feature Engineering ---
print("Engineering features for ML...")
df_merged['tempc_anomaly_prev_year'] = df_merged.groupby('state_abbr')['tempc_anomaly'].shift(1)
df_merged['disaster_count_prev_year'] = df_merged.groupby('state_abbr')['disaster_count'].shift(1)
df_merged['target_tempc_anomaly_next_year'] = df_merged.groupby('state_abbr')['tempc_anomaly'].shift(-1)

df_model_data = df_merged.dropna(subset=[
    'tempc_anomaly', 'tempc_anomaly_prev_year', 'disaster_count',
    'disaster_count_prev_year', 'target_tempc_anomaly_next_year'
]).copy()

# Sort by year for TimeSeriesSplit
df_model_data = df_model_data.sort_values(by='year')

features = ['year', 'tempc_anomaly', 'tempc_anomaly_prev_year', 'disaster_count', 'disaster_count_prev_year']
target = 'target_tempc_anomaly_next_year'

X = df_model_data[features]
y = df_model_data[target]

# --- 4. Machine Learning Model & Time Series Cross-Validation ---
print(f"Performing Time Series Cross-Validation with {N_CV_SPLITS} splits...")

tscv = TimeSeriesSplit(n_splits=N_CV_SPLITS)
model = Ridge(alpha=1.0)
rmse_scores = []

fold = 0
for train_index, val_index in tscv.split(X):
    fold += 1
    X_train, X_val = X.iloc[train_index], X.iloc[val_index]
    y_train, y_val = y.iloc[train_index], y.iloc[val_index]

    # Handle potential NaNs introduced within splits if any (unlikely with prior dropna but safe)
    X_train = X_train.fillna(X_train.median())
    X_val = X_val.fillna(X_train.median()) # Fill val NaNs with train median

    model.fit(X_train, y_train)
    y_pred_val = model.predict(X_val)
    rmse = np.sqrt(mean_squared_error(y_val, y_pred_val))
    rmse_scores.append(rmse)
    print(f"Fold {fold}/{N_CV_SPLITS} - Validation RMSE: {rmse:.4f}")

print(f"\nAverage Cross-Validation RMSE: {np.mean(rmse_scores):.4f} +/- {np.std(rmse_scores):.4f}")

# --- Retrain final model on ALL available data for prediction ---
print("\nRetraining final model on all available data...")
# Ensure no NaNs in the final training data (should be handled by dropna earlier)
X = X.fillna(X.median())
model.fit(X, y)
print("Final model trained.")


# --- 5. Iterative Prediction for Next Decade & Risk Calculation ---
print(f"\nPredicting next {PREDICTION_HORIZON_YEARS} years' anomaly and calculating risk...")

# Get the latest data row for each state to start the prediction
last_year_data_full = df_merged.loc[df_merged.groupby('state_abbr')['year'].idxmax()]
last_known_year = last_year_data_full['year'].max()
print(f"Last known year in data: {last_known_year}")
print(f"Predicting from {last_known_year + 1} to {last_known_year + PREDICTION_HORIZON_YEARS}")

# Initialize DataFrame to store predictions
future_predictions = {} # Dict to store state-specific predictions

# Prepare the initial input for the first prediction step (Year N+1)
# Using the actual last available data
current_features = last_year_data_full.set_index('state_abbr')[[
    'year', 'tempc_anomaly', 'tempc_anomaly_prev_year',
    'disaster_count', 'disaster_count_prev_year'
]].copy()

# Fill NaNs in initial features (e.g., if a state only has 1 year of data)
feature_medians = X.median() # Use medians from the full training set
for col in features:
    if current_features[col].isnull().any():
        fill_value = feature_medians[col]
        print(f"Warning: Filling initial NaN in '{col}' with median {fill_value:.2f}")
        current_features[col] = current_features[col].fillna(fill_value)


# Iteratively predict for the horizon
all_states_in_prediction = current_features.index.unique()
predicted_anomalies_decade = {} # Store final 10th year prediction

for i in range(PREDICTION_HORIZON_YEARS):
    predict_yr = last_known_year + 1 + i
    current_features['year'] = predict_yr

    # Ensure columns are in the exact order the model expects
    X_step = current_features[features]

    # Predict anomaly for the current step year
    step_predictions = model.predict(X_step)
    predicted_anomaly_series = pd.Series(step_predictions, index=X_step.index)

    # Store the prediction for this step (needed for next step's input)
    future_predictions[predict_yr] = predicted_anomaly_series

    # Prepare features for the *next* iteration
    # The predicted anomaly becomes the 'current' anomaly for the next step
    # The 'current' anomaly becomes the 'previous' anomaly for the next step
    next_tempc_anomaly_prev = current_features['tempc_anomaly'].copy() # Store before overwriting
    current_features['tempc_anomaly'] = predicted_anomaly_series
    current_features['tempc_anomaly_prev_year'] = next_tempc_anomaly_prev

    # Keep disaster counts constant at their last known values (simplification)
    # They are already in current_features and don't need updating in this loop

    # Store the final prediction (10 years out)
    if i == PREDICTION_HORIZON_YEARS - 1:
        predicted_anomalies_decade = predicted_anomaly_series.copy()

# Get the prediction for the very next year (Year N+1) for risk score calculation
predicted_anomalies_next_year = future_predictions[last_known_year + 1]

# --- Risk Score Calculation (Based on NEXT year's prediction) ---
recent_disaster_avg = df_merged[df_merged['year'] > last_known_year - 5]\
    .groupby('state_abbr')['disaster_count'].mean().fillna(0)

risk_components = pd.DataFrame({
    'predicted_anomaly': predicted_anomalies_next_year, # Use next year's prediction
    'recent_disasters': recent_disaster_avg
}).reindex(fips_to_state_abbr.values()).fillna(0)

scaler = MinMaxScaler()
risk_components_scaled = scaler.fit_transform(risk_components)
risk_components_scaled = pd.DataFrame(risk_components_scaled,
                                      columns=['anomaly_scaled', 'disasters_scaled'],
                                      index=risk_components.index)

weight_anomaly = 0.6
weight_disasters = 0.4
risk_components_scaled['risk_score'] = (weight_anomaly * risk_components_scaled['anomaly_scaled'] +
                                        weight_disasters * risk_components_scaled['disasters_scaled'])
risk_components_scaled['risk_score'] = (risk_components_scaled['risk_score'] * 100).round(1)

quantiles = risk_components_scaled['risk_score'].quantile([0.2, 0.4, 0.6, 0.8]).to_dict()
def get_risk_category_and_color(score):
    # Handle potential edge case where quantiles might be equal if data is sparse/uniform
    q = sorted(quantiles.values())
    if score <= q[0]: return "Very Low", "#4575b4"
    elif score <= q[1]: return "Low", "#91bfdb"
    elif score <= q[2]: return "Medium", "#ffffbf"
    elif score <= q[3]: return "High", "#fc8d59"
    else: return "Very High", "#d73027"

risk_categories_colors = risk_components_scaled['risk_score'].apply(get_risk_category_and_color)
risk_components_scaled['riskCategory'] = risk_categories_colors.apply(lambda x: x[0])
risk_components_scaled['color'] = risk_categories_colors.apply(lambda x: x[1])


# --- 6. Output Generation ---
print(f"\nGenerating JavaScript output file: {OUTPUT_JS_FILE}")
climate_risk_data_dict = {}
all_states = fips_to_state_abbr.values()

for state_abbr in all_states:
    state_data = {
        "riskScore": "N/A",
        "riskCategory": "No Data",
        "color": "#333333",
        "topDisasterType": "No Data",
        "totalDisasters": 0,
        "nextYearTrend": "N/A",
        "decadeAnomalyPrediction": "N/A" # New field
    }

    if state_abbr in risk_components_scaled.index:
        risk_info = risk_components_scaled.loc[state_abbr]
        state_data["riskScore"] = risk_info['risk_score']
        state_data["riskCategory"] = risk_info['riskCategory']
        state_data["color"] = risk_info['color']

    if state_abbr in predicted_anomalies_next_year.index:
        trend_val = predicted_anomalies_next_year.loc[state_abbr]
        state_data["nextYearTrend"] = f"{trend_val:+.2f}°C Anomaly (1yr)"

    # Add the decade prediction
    if state_abbr in predicted_anomalies_decade.index:
        decade_val = predicted_anomalies_decade.loc[state_abbr]
        state_data["decadeAnomalyPrediction"] = f"{decade_val:+.2f}°C Anomaly ({PREDICTION_HORIZON_YEARS}yr)"
    else:
         # Handle cases where prediction might have failed for a state if initial data was bad
         state_data["decadeAnomalyPrediction"] = "Prediction Error"


    if state_abbr in top_disasters['state_abbr'].values:
         state_data["topDisasterType"] = top_disasters.loc[top_disasters['state_abbr'] == state_abbr, 'top_disaster_type'].iloc[0]

    total_hist_disasters = df_fema[df_fema['state_abbr'] == state_abbr].shape[0]
    state_data["totalDisasters"] = total_hist_disasters

    climate_risk_data_dict[state_abbr] = state_data

# Convert dictionary to JSON string, then format as JavaScript variable
json_string = json.dumps(climate_risk_data_dict, indent=4)
js_output = f"const climateRiskData = {json_string};"

# Write to file
try:
    with open(OUTPUT_JS_FILE, 'w') as f:
        f.write(js_output)
    print(f"Successfully created {OUTPUT_JS_FILE}")
except IOError as e:
    print(f"Error writing to file {OUTPUT_JS_FILE}: {e}")

print("\nScript finished.")